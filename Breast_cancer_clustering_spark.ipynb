{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcxew4hv5mxo"
   },
   "source": [
    "# **K Means using Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to install pyspark to work with big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dl8tPfMC5MRY",
    "outputId": "0d2497b8-8e5c-467a-f3d2-80ea4cc46b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 48 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 51.8 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=13f65afda5a36ad75a736dfd5d1ca8174ee711ebaa3e3c20ad7dc83261652bad\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_ap5a0lm54Ny"
   },
   "outputs": [],
   "source": [
    "#updating pydrive\n",
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JQkuJS54SZ",
    "outputId": "6b9982d4-b670-4819-9e58-609b26198b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following additional packages will be installed:\n",
      "  openjdk-8-jre-headless\n",
      "Suggested packages:\n",
      "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
      "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
      "0 upgraded, 2 newly installed, 0 to remove and 22 not upgraded.\n",
      "Need to get 36.6 MB of archives.\n",
      "After this operation, 143 MB of additional disk space will be used.\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "(Reading database ... 123941 files and directories currently installed.)\n",
      "Preparing to unpack .../openjdk-8-jre-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jdk-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
     ]
    }
   ],
   "source": [
    "!apt install openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code setup a directory \n",
    "\n",
    "OpenJDK is an open source implementation of the Java Platform, Standard Edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lmohVHXN54Xe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Importing necessary libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ErayPprJ54ch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Creating the session and context***\n",
    "\n",
    "SparkSession is a class that is part of pyspark.sql package.\n",
    "\n",
    "It is a wrapper on top of Spark Context.\n",
    "\n",
    "When Spark application is submitted using spark-submit or spark-shell or pyspark, a web service called as Spark Context will be started.\n",
    "\n",
    "Spark Context maintains the context of all the jobs that are submitted until it is killed.\n",
    "\n",
    "SparkSession is nothing but wrapper on top of Spark Context.\n",
    "\n",
    "We need to first create SparkSession object with any name. But typically we use spark. Once it is created, several APIs will be exposed including read.\n",
    "\n",
    "We need to at least set Application Name and also specify the execution mode in which Spark Context should run while creating SparkSession object.\n",
    "\n",
    "We can use appName to specify name for the application and master to specify the execution mode.\n",
    "\n",
    "Below is the sample code snippet which will start the Spark Session object for us.\n",
    "\n",
    "**SparkConf()**\n",
    "\n",
    "Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.\n",
    "\n",
    "Most of the time, you would create a SparkConf object with SparkConf(), which will load values from spark.* Java system properties as well. In this case, any parameters you set directly on the SparkConf object take priority over system properties.\n",
    "\n",
    "For unit tests, we can also call SparkConf(false) to skip loading external settings and get the same configuration no matter what the system properties are.\n",
    "\n",
    "All setter methods in this class support chaining. \n",
    "\n",
    "\n",
    "Spark User Interface, which shows application’s dashboard, has the default port of 4040 (link). Property name is **spark.ui.port**\n",
    "\n",
    "--conf spark.ui.port=4050 is a Spark 1.1 feature\n",
    "\n",
    "\n",
    "**Port numbers** in computer networking represent communication endpoints. Ports are unsigned 16-bit integers (0-65535) that identify a specific process, or network service. IANA is responsible for internet protocol resources, including the registration of commonly used port numbers for well-known internet services.\n",
    "\n",
    "Well Known Ports: 0 through 1023.\n",
    "\n",
    "Registered Ports: 1024 through 49151.\n",
    "\n",
    "Dynamic/Private : 49152 through 65535.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Tcga9hJS54hT"
   },
   "outputs": [],
   "source": [
    "# Create the session\n",
    "conf = SparkConf().set('spark.ui.port','4050')\n",
    "\n",
    "# Create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily check the current version and get the link of the web interface. In the Spark UI, we can monitor the progress of our job and debug the performance bottlenecks(if our colab is running with a local run time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "EYl5okBS54qZ",
    "outputId": "901958a9-a200-481d-a79f-a987add333f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c02a340fd2f8:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f02ba6304d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to know the version of pyspark\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are running this colab on the Gooogle hosted runtime, the cell belowwill create a ngrok tunnel which will allow us to still check the Spark\n",
    "\n",
    "### What is Ngrok?\n",
    "Ngrok is an application developed by Alan Shreeve, enables developers to expose their local development servers to the internet. It basically creates a tunnel to your local development server and generates two random subdomains on ngrok.com - one http and another with https. With these generated addresses, you can view the locally developed application from anywhere through the Internet provided that the development server is kept running.\n",
    "\n",
    "Other than just demoing developed applications, ngrok can also be used for :-\n",
    "\n",
    "* Running a personal service of your own on the cloud.\n",
    "* Testing out mobile apps that have a local backend server.\n",
    "* Building and testing out webhooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PneBbolY54wK",
    "outputId": "bfa978d8-f7cd-4717-ea44-53b7ec74216b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-19 11:19:29--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 54.161.241.46, 52.202.168.65, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13832437 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.19M  17.4MB/s    in 0.8s    \n",
      "\n",
      "2022-10-19 11:19:31 (17.4 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "get_ipython().system_raw('./ngrok http 4050 &')\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "In this colab, rather than downloading a file from Google Drive, we will load a famous machine learning dataset, the **Breast Cancer Wisconsin dataset** using the scikit-learn dataset loader.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "   Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei.\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus (3–32):\n",
    "\n",
    "a) radius (mean of distances from the center to points on the perimeter)\n",
    "\n",
    "b) texture (standard deviation of gray-scale values)\n",
    "\n",
    "c) perimeter\n",
    "\n",
    "d) area\n",
    "\n",
    "e) smoothness (local variation in radius lengths)\n",
    "\n",
    "f) compactness (perimeter² / area — 1.0)\n",
    "\n",
    "g) concavity (severity of concave portions of the contour)\n",
    "\n",
    "h) concave points (number of concave portions of the contour)\n",
    "\n",
    "i) symmetry\n",
    "\n",
    "j) fractal dimension (“coastline approximation” — 1)\n",
    "\n",
    "### what is KMean?\n",
    "   The algorithm we will use to perform segmentation analysis is K-Means clustering. K-Means is a partitioned based algorithm that performs well on medium/large datasets. The algorithm is an unsupervised learning algorithm that utilizes similarities in the data to create groups without labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5QyFv1U--J63"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, given that the dataset is small, we first construct a pandas dataframe, tune the schema, and then convert it into a Spark dataframe.\n",
    "\n",
    "While creating a PySpark DataFrame we can specify the structure using StructType and StructField classes. As specified in the introduction, StructType is a collection of StructField’s which is used to define the column name, data type, and a flag for nullable or not. Using StructField we can also add nested struct schema, ArrayType for arrays, and MapType for key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f7j4aft-ngJ",
    "outputId": "a8ee8b30-b3a6-4b0a-d548-1350e5a436aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mean radius: double (nullable = false)\n",
      " |-- mean texture: double (nullable = false)\n",
      " |-- mean perimeter: double (nullable = false)\n",
      " |-- mean area: double (nullable = false)\n",
      " |-- mean smoothness: double (nullable = false)\n",
      " |-- mean compactness: double (nullable = false)\n",
      " |-- mean concavity: double (nullable = false)\n",
      " |-- mean concave points: double (nullable = false)\n",
      " |-- mean symmetry: double (nullable = false)\n",
      " |-- mean fractal dimension: double (nullable = false)\n",
      " |-- radius error: double (nullable = false)\n",
      " |-- texture error: double (nullable = false)\n",
      " |-- perimeter error: double (nullable = false)\n",
      " |-- area error: double (nullable = false)\n",
      " |-- smoothness error: double (nullable = false)\n",
      " |-- compactness error: double (nullable = false)\n",
      " |-- concavity error: double (nullable = false)\n",
      " |-- concave points error: double (nullable = false)\n",
      " |-- symmetry error: double (nullable = false)\n",
      " |-- fractal dimension error: double (nullable = false)\n",
      " |-- worst radius: double (nullable = false)\n",
      " |-- worst texture: double (nullable = false)\n",
      " |-- worst perimeter: double (nullable = false)\n",
      " |-- worst area: double (nullable = false)\n",
      " |-- worst smoothness: double (nullable = false)\n",
      " |-- worst compactness: double (nullable = false)\n",
      " |-- worst concavity: double (nullable = false)\n",
      " |-- worst concave points: double (nullable = false)\n",
      " |-- worst symmetry: double (nullable = false)\n",
      " |-- worst fractal dimension: double (nullable = false)\n",
      " |-- features: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "df = spark.createDataFrame(pd_df)\n",
    "\n",
    "'''PySpark StructType & StructField classes are used to programmatically specify the schema to the DataFrame and create complex columns like nested struct, array, and map columns. StructType is a collection of StructField’s that defines column name, \n",
    "column data type, boolean to specify if the field can be nullable or not and metadata.'''\n",
    "\n",
    "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
    "    for struct_field in df.schema:  #StructField – Defines the metadata of the DataFrame column\n",
    "        if struct_field.name in column_list:\n",
    "            struct_field.nullable = nullable\n",
    "            '''nullable argument is not a constraint but a reflection of the source and type semantics which enables \n",
    "            certain types of optimization we state that we want to avoid null values in your data.'''\n",
    "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
    "    return df_mod\n",
    "\n",
    "df = set_df_columns_nullable(spark, df, df.columns)\n",
    "df = df.withColumn('features', array(df.columns))\n",
    "vectors = df.rdd.map(lambda row: Vectors.dense(row.features)) #converting the dataframe to an RDD of tuples\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next cell, we build the two datastructures that we will be using throughout this colab:\n",
    "* features, a dataframe of Dense vectors, containing all the original features in the dataset;\n",
    "* labels, a series of binary labels indicating if the corresponding set of features belong to a subject with breast cancer, or not.\n",
    "\n",
    "### what is data structure?\n",
    "A data structure is a storage that is used to store and organize data. It is a way of arranging data on a computer so that it can be accessed and updated efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BfPs0VIQB9sR"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "features = spark.createDataFrame(vectors.map(Row),['features'])\n",
    "labels = pd.Series(breast_cancer.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "PySpark kmeans is a method and function used in the PySpark Machine learning model that is a type of unsupervised learning where the data is without categories or groups. Instead, it groups up the data together and assigns data points to them. This model approach is used for prediction or machine learning analysis of data in the PySpark machine learning model. We have a various model in PySpark that is used to import the data elements and to apply the kmeans algorithm logic to it. Furthermore, the data can be used further for prediction by applying the vector assembler that is a part of the PySpark kmeans data model. Here we will analyze the various method used in kmeans with the data in PySpark.\n",
    "\n",
    "## Silhoutte score\n",
    "Evaluator for Clustering results, which expects two input columns: prediction and features. The metric computes the Silhouette measure using the squared Euclidean distance.\n",
    "\n",
    "The Silhouette is a measure for the validation of the consistency within clusters. It ranges between 1 and -1, where a value close to 1 means that the points in a cluster are close to the other points in the same cluster and far from the points of the other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-m71i3izB9v2"
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries for kmean algorithm and evaluating it\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7QDtvUi7B9z0"
   },
   "outputs": [],
   "source": [
    "# Trains a k-means model\n",
    "kmeans = KMeans().setK(2).setSeed(1)  #The number 2 is the number of clusters to divide the data into.\n",
    "\n",
    "'''We see that any number larger than 2 causes this value ClusteringEvaluator() to fall below 0.5, meaning it’s not a clear division. \n",
    "Another way to check the optimal number of clusters would be to plot an elbow curve.'''\n",
    "\n",
    "model = kmeans.fit(features) #To train our model , we use kmeans.fit() here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K4CqKJZ0KWDJ"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.transform(features)  #transforms from one parameter to others dependent on the model\n",
    "\n",
    "# Evaluate clustering by computing silhoutte score\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters.\n",
    "\n",
    "* Silhouette score for a set of sample data points is used to measure how dense and well-separated the clusters are.\n",
    "* Silhouette score takes into consideration the intra-cluster distance between the sample and other data points within the same cluster (a) and inter-cluster distance between the sample and the next nearest cluster (b).\n",
    "* The silhouette score falls within the range [-1, 1].\n",
    "* The silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score of less than 0 means that data belonging to clusters may be wrong/incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ur9d9PsHKupb",
    "outputId": "2ffb0f75-5afa-447a-935b-392b8dc4e71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance= 0.8342904262826145\n"
     ]
    }
   ],
   "source": [
    "silhouette = evaluator.evaluate(predictions)\n",
    "print('Silhouette with squared euclidean distance= '+ str(silhouette))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaQUWwvEKuth"
   },
   "source": [
    "silhoutte score indicates that clusters are dense and nicely separated since it has value nearer to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmsAbGvfKuxE"
   },
   "source": [
    "# _THANK YOU!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afBylWuIKu0d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUp5fHKaKWKL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssiDmujqKWOf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
